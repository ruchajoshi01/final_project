---
title: "The Office"
author: "Rucha Joshi"
date: "12/2/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)

library(tidyverse)
library(tidytext)
library(googlesheets4)
library(RCurl)
library(SentimentAnalysis)
library(janitor)
library(reshape2)
library(wordcloud)
library(moderndive)
library(psych)
library(ggcorrplot)
library(scales)
library(plotly)
library(janitor)
```

# Cleaned Data
```{r}
# Read in and clean data
sheets_deauth()
clean_data <- read_sheet("13ZZfByyxT2hznhdALJtQqbslW6VYERlJsRbumOQuTYA") %>%
    filter(deleted == "FALSE") %>%
    mutate(actions = str_extract_all(line_text, "\\[.*?\\]"),
           line_text_mod = str_trim(str_replace_all(line_text, "\\[.*?\\]", ""))) %>% 
    mutate_at(vars(line_text_mod), funs(str_replace_all(., "���","'"))) %>% 
    mutate_at(vars(speaker), funs(tolower)) %>% 
    mutate_at(vars(speaker), funs(str_trim(str_replace_all(., "\\[.*?\\]", "")))) %>% 
    mutate_at(vars(speaker), funs(str_replace_all(., "micheal|michel|michae$", "michael")))

# Create clean data rds file for Shiny app
clean_data_df <- as.data.frame(clean_data)
write_rds(clean_data_df, "./the-office/clean_data.rds")

# Read in and clean data on IMDB ratings
sheets_deauth()
imdb_data <- read_sheet("1TkOAXgSaN7PNxzNVtJGMQvClSYVZyy_ff1NqY54uiCw") %>%
  clean_names()

# Create IMDB ratings data rds file for Shiny app
imdb_data <- as.data.frame(imdb_data)
write_rds(imdb_data, "./the-office/imdb_data.rds")

```

# Relationships/ Correlations
```{r}
# Words we don't want to include because they are too generic
generic_words <- bind_rows(data_frame(word = c("yeah", "hey", "uh", "um", 
                                               "huh", "hmm", "ah", "umm", 
                                               "uhh", "gonna", "na", "ha", 
                                               "gotta"), 
                                          lexicon = c("custom")), 
                               stop_words)

# Individual words without the words we don't want
tidy_tokens_no_stop2 <- tidy_tokens %>% 
  anti_join(generic_words, by = "word")


frequency_by_character <- tidy_tokens_no_stop2 %>%
  filter(speaker %in% c("toby", "stanley", "ryan", "roy", 
                        "phyllis", "pam", "oscar", "michael", "holly", 
                        "dwight", "meredith", "kelly", "kevin", "jim", 
                        "jan", "angela", "darryl")) %>% 
  count(speaker, word, sort = TRUE) %>% 
  group_by(speaker) %>% 
  mutate(proportion = n / sum(n)) %>% 
  select(-n) %>% 
  spread(speaker, proportion) 

# Create rds file for Shiny app
frequency_by_character <- as.data.frame(frequency_by_character)
write_rds(frequency_by_character, "./the-office/frequency_by_character.rds")

```

```{r}
########################################################################################
# EXAMPLE PLOTS

# I used these plots to help me figure out how to write the equivalent plots in the app.R file

# Plot number of lines spoken by each character in season 1
# plot1 <- clean_data %>%
#   select(season, episode, speaker) %>%
#   filter(season == "1", speaker == c("toby", "stanley", "ryan", "roy",
#                                     "phyllis", "pam", "oscar", "michael",
#                                     "dwight", "meredith", "kevin", "jim",
#                                     "jan", "angela", "darryl")) %>%
#   ggplot(aes(x = speaker)) +
#   geom_bar() +
#   labs(title = "Number of lines spoken by each character in season 1",
#        x = "Character",
#        y = "Number of lines") +
#   coord_flip()

# Plot number of episodes each character is in per season
# plot2 <- clean_data %>%
#   select(season, episode, speaker) %>%
#   filter(season == "1", speaker == c("toby", "stanley", "ryan", "roy", 
#                                     "phyllis", "pam", "oscar", "michael", 
#                                     "dwight", "meredith", "kevin", "jim", 
#                                     "jan", "angela", "darryl")) %>% 
#   count(episode, speaker) %>%
#   ggplot(aes(x = speaker)) +
#   geom_bar() +
#   labs(title = "Number of episodes acted by each character in season 1", 
#        x = "Character", 
#        y = "Number of episodes") +
#   coord_flip()

# Plot number of scenes per character
# plot3 <- clean_data %>%
#   select(season, speaker, scene) %>%
#   filter(season == "1", speaker == c("toby", "stanley", "ryan", "roy", 
#                                     "phyllis", "pam", "oscar", "michael", 
#                                     "dwight", "meredith", "kevin", "jim", 
#                                     "jan", "angela", "darryl")) %>% 
#   count(scene, speaker) %>%
#   ggplot(aes(x = speaker)) +
#   geom_bar() +
#   labs(title = "Number of scenes acted by each character in season 1", 
#        x = "Character", 
#        y = "Number of scenes") +
#   coord_flip()
########################################################################################

########################################################################################

# EXAMPLE PLOTS 

# I used these plots to help me understand what kind of graphs I want to display
# in my app.R file.

# Create bar plot of most frequent words for each speaker
# ggplot(tidy_tokens_no_stop, aes(x = reorder(word, prop), y = prop)) + 
#   geom_col() +
#   coord_flip() +
#   labs(title = "word frequencies: Michael",
#        y = "Percentages",
#        x = "")

# Create data frame of most unique and frequent words for each speaker
# tidy_tokens_tf_idf <- tidy_tokens %>%
#   count(speaker, word, sort = TRUE) %>%
#   ungroup() %>% 
#   filter(speaker %in% c("toby", "stanley", "ryan", "roy", 
#                                     "phyllis", "pam", "oscar", "michael", 
#                                     "dwight", "meredith", "kevin", "jim", 
#                                     "jan", "angela", "darryl")) %>% 
#   bind_tf_idf(word, speaker, n) %>%
#   arrange(desc(tf_idf)) %>%
#   rename(prop = tf_idf) %>%
#   filter(speaker == "michael") %>%
#   head(10)

# # Create bar plot of most unique and frequent words for each speaker
# ggplot(tidy_tokens_tf_idf, aes(x = reorder(word, prop), y = prop)) +
#   geom_col() +
#   coord_flip() +
#   labs(title = "word frequencies: Michael",
#        y = "Percentages",
#        x = "")

# nrc_joy <- get_sentiments("nrc")
# 
# michael_lines_1 <- clean_data %>%
#   filter(speaker == "michael") %>%
#   filter(season == 1) %>%
#   select(line = id, line_text_mod, everything(), -line_text, -actions, -deleted) %>% 
#   unnest_tokens(word, line_text_mod, strip_numeric = TRUE) %>%
#   mutate_at(vars(word), funs(str_replace_all(., "'s$", ""))) %>%
#   inner_join(nrc_joy) %>%
#   group_by(sentiment) %>%
#   count(sentiment, sort = TRUE)
#   #rowid_to_column("id")
#   #summarise(word_count = n())

#michael_lines_2 <- tibble(michael_lines_1$sentiment, michael_lines_1$n, id = 1:10)
#unnest(michael_lines_1) 
# label_data = michael_lines_1
# number_of_bar = nrow(label_data) #Calculate the ANGLE of the labels
# angle = 90 - 360 * (label_data$id - 0.5) / number_of_bar #Center things
# label_data$hjust <- ifelse(angle < -90, 1, 0) #Align label
# label_data$angle <- ifelse(angle < -90, angle + 180, angle) #Flip angle
# 
# 
# bar_plot <- ggplot(michael_lines_1, aes(x = "", y = n, fill = sentiment)) + 
#   geom_bar(width = 1, stat = "identity") + 
#   labs(x = "number",
#        y = "sentiment")
# 
# pie <- bar_plot + coord_polar("y", start=0)
# 
# bing <- get_sentiments("bing")
# 
# bing_analysis <- clean_data %>%
#   filter(speaker == "angela") %>%
#   filter(season == 9) %>%
#   select(line = id, line_text_mod, everything(), -line_text, -actions, -deleted) %>% 
#   unnest_tokens(word, line_text_mod, strip_numeric = TRUE) %>%
#   mutate_at(vars(word), funs(str_replace_all(., "'s$", ""))) %>%
#   group_by(episode, speaker) %>%
#   inner_join(bing) %>%
#  # group_by(sentiment) %>%
#   count(sentiment, sort = TRUE)
# 
# line_bing <- bing_analysis %>%
#   ggplot(aes(x = episode, y = n, group = sentiment, color = sentiment)) +
#   geom_line()
# 
# 
# afinn <- get_sentiments("afinn")
# 
# afinn_analysis <- clean_data %>%
#   filter(speaker == "stanley") %>%
#   filter(season == 2) %>%
#   select(line = id, line_text_mod, everything(), -line_text, -actions, -deleted) %>% 
#   unnest_tokens(word, line_text_mod, strip_numeric = TRUE) %>%
#   mutate_at(vars(word), funs(str_replace_all(., "'s$", ""))) %>%
#   group_by(episode, speaker) %>%
#   inner_join(afinn) %>%
#  # group_by(sentiment) %>%
#   count(value, sort = TRUE)
# 
# line_afinn <- afinn_analysis %>%
#   ggplot(aes(x = episode, y = value, group = value, color = value)) +
#   geom_point()
# 
# 
# # Word Clouds
# word_clouds_senti <- tidy_tokens %>%
#   inner_join(get_sentiments("bing")) %>%
#   count(word, sentiment, sort = TRUE) %>%
#   acast(word ~ sentiment, value.var = "n", fill = 0) %>%
#   comparison.cloud(colors = c("gray20", "gray80"),
#                    max.words = 100)
# 
# word_clouds <- tidy_tokens %>%
#   count(word) %>%
#   with(wordcloud(word, n, max.words = 100))

########################################################################################

# EXAMPLE PLOTS

# I used these plots to create regressions for different kinds of information, such as whether 
# there is a correlation between being a woman and using negative words (there is not a correlation)

# df['elderly'] = np.where(df['age']>=50, 'yes', 'no')
# 
# reg_data <- clean_data_df
# 
# reg_data$gender = ifelse(reg_data$speaker %in% c("toby", "stanley", "ryan", "roy", "oscar", 
#                                                        "michael", "dwight", "kevin", "jim", "darryl"), 0, 1)
# 
# reg_token <- reg_data %>%
#   select(line = id, line_text_mod, everything(), -line_text, -actions, -deleted) %>% 
#   unnest_tokens(word, line_text_mod, strip_numeric = TRUE) %>%
#   mutate_at(vars(word), funs(str_replace_all(., "'s$", ""))) %>%
#   anti_join(stop_words, by = "word") %>%
#   inner_join(afinn)
# 
# correlation <- reg_token %>%
#   get_correlation(formula = gender ~ value)
# 
# ggplot(reg_token, aes(x = value, y = gender)) +
#   geom_point() +
#   geom_smooth(method = "lm", se = FALSE) +
#   labs(x = "Value",
#        y = "Gender",
#        title = "Relationship between gender and value of words")

###############################################################################################

# Experimenting with TidyText

# # Isolate each individual word in data
# tidy_tokens <- clean_data %>%
#   select(line = id, line_text_mod, everything(), -line_text, -actions, -deleted) %>% 
#   unnest_tokens(word, line_text_mod, strip_numeric = TRUE) %>%
#   mutate_at(vars(word), funs(str_replace_all(., "'s$", ""))) %>%
#   anti_join(stop_words, by = "word")
# 
# # Words we don't want in the data because they are too generic
# custom_stop_words <- bind_rows(data_frame(word = c("yeah", "hey", "uh", "um", "huh", "hmm", "ah", "umm", "uhh", "gonna", "na", "ha", "gotta"), 
#                                           lexicon = c("custom")), 
#                                stop_words)
# 
# # Most frequent words for each speaker
# tidy_tokens_no_stop <- tidy_tokens %>% 
#   anti_join(custom_stop_words, by = "word") %>%
#   filter(season == 1) %>%
#   count(speaker, word) %>%
#   arrange(desc(n)) %>%
#   mutate(prop = round((n / sum(n))*100, 1)) %>%
#   head(10)

# Experimenting with word correlations

# cor_all <- corr.test(frequency_by_character[, -1], adjust = "none")
# cor_plot <- ggcorrplot(cor_all[["r"]], 
#                         hc.order = TRUE, 
#                         type = "lower",
#                         method = "circle",
#                         lab = TRUE,
#                         lab_size = 2.5) + 
#    labs(title = "Correlation between Word Frequencies")
# 
# cor_plot
# 
# pam_jim_words <- frequency_by_character %>% 
#   select(word, pam, jim) %>% 
#   ggplot(aes(x = pam, y = jim, color = abs(pam - jim), label = word)) +
#   geom_abline(color = "gray40", lty = 2) +
#   geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
#   scale_x_log10(labels = percent_format()) +
#   scale_y_log10(labels = percent_format()) +
#   labs(x = "Pam",
#        y = "Jim",
#        title = "Word Frequncy Comparison: Pam and Jim") +
#   theme(legend.position = "none")
# 
# ggplotly(pam_jim_words, tooltip = c("word"))
# 
# angela_dwight_words <- frequency_by_character %>% 
#   select(word, angela, dwight) %>% 
#   ggplot(aes(x = angela, y = dwight, color = abs(angela - dwight), label = word)) +
#   geom_abline(color = "gray40", lty = 2) +
#   geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
#   scale_x_log10(labels = percent_format()) +
#   scale_y_log10(labels = percent_format()) +
#   labs(x = "Angela",
#        y = "Dwight",
#        title = "Word Frequncy Comparison: Angela and Dwight") +
#   theme(legend.position = "none")
# 
# ggplotly(angela_dwight_words, tooltip = c("word"))
```

